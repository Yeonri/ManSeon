# Infra

## 1. 환경

- **리눅스 서버 4대:** 서버 간 네트워크 통신 가능. 각 서버의 IP 주소 확인.
- **SSH 접속:** 각 서버 sudo 권한 필요.
- **Docker & Docker Compose:** 모든 서버에 최신 버전 설치.
- **방화벽 설정:** 필요한 포트 오픈 (아래 'Docker Compose 파일 작성' 참조).

## 2. 프로젝트 구조 설정 (기준 서버)

클러스터 관리를 위한 기준 서버(예: NameNode/ResourceManager 서버)에서 실행.

```python
mkdir -p ~/hadoop-cluster/{config,data,ssh-keys}
cd ~/hadoop-cluster
```

**데이터 디렉토리 준비 (각 서버):**

```python
# NameNode/RM 서버
mkdir -p ~/hadoop-cluster/data/namenode

# Worker1 서버
mkdir -p ~/hadoop-cluster/data/datanode-w1
mkdir -p ~/hadoop-cluster/data/nm-local-w1
mkdir -p ~/hadoop-cluster/data/nm-logs-w1

# Worker2 서버
mkdir -p ~/hadoop-cluster/data/datanode-w2
mkdir -p ~/hadoop-cluster/data/nm-local-w2
mkdir -p ~/hadoop-cluster/data/nm-logs-w2

# SNN/API 서버 (필요시)
# mkdir -p ~/hadoop-cluster/data/secondarynamenode
```

## **3. SSH 키 설정**

<aside>

1. **SSH 키 생성 (기준 서버):**

```python
ssh-keygen -t rsa -P "" -f ~/hadoop-cluster/ssh-keys/id_rsa
```

1. **키 준비:**
    - ssh-keys/id_rsa.pub: Docker 이미지 빌드 시 사용.
    - ssh-keys/id_rsa: NameNode 컨테이너에 마운트.
</aside>

# **4. 설정 파일 구성 (~/hadoop-cluster/config/)**

**core-site.xml**

```python
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://namenode:9000</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/opt/hadoop/tmp</value>
  </property>
  <property>
    <name>dfs.client.use.datanode.hostname</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.datanode.use.datanode.hostname</name>
    <value>true</value>
  </property>
</configuration>
```

**hdfs-site.xml**

```python
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///opt/hadoop/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///opt/hadoop/hdfs/datanode</value>
  </property>
  <property>
    <name>dfs.namenode.http-address</name>
    <value>namenode:9870</value>
  </property>
  <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>secondarynamenode:9868</value>
  </property>
  <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.namenode.checkpoint.period</name>
    <value>3600</value>
  </property>
  <property>
    <name>dfs.namenode.checkpoint.txns</name>
    <value>1000000</value>
  </property>
</configuration>
```

**yarn-site.xml**

```python
<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>resourcemanager</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>12288</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>3</value>
  </property>
  <property>
    <name>yarn.nodemanager.local-dirs</name>
    <value>/opt/hadoop/tmp/nmlocaldir</value>
  </property>
  <property>
    <name>yarn.nodemanager.log-dirs</name>
    <value>/opt/hadoop/logs/userlogs</value>
  </property>
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>604800</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>resourcemanager:8088</value>
  </property>
</configuration>
```

**mapred-site.xml**

```python
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.address</name>
    <value>resourcemanager:10020</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>resourcemanager:19888</value>
  </property>
  <property>
      <name>yarn.app.mapreduce.am.env</name>
      <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  </property>
  <property>
      <name>mapreduce.map.env</name>
      <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  </property>
  <property>
      <name>mapreduce.reduce.env</name>
      <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
  </property>
</configuration>
```

**workers**

```python
worker1
worker2
```

# 5. **Dockerfile 작성**

**Dockerfile.hadoop-base (Hadoop + Spark)**

```python
FROM openjdk:8-jdk

ENV HADOOP_VERSION=3.3.6 \
    SPARK_VERSION=3.5.5 \
    HADOOP_HOME=/opt/hadoop \
    SPARK_HOME=/opt/spark \
    HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop \
    SPARK_CONF_DIR=/opt/spark/conf \
    JAVA_HOME=/usr/local/openjdk-8 \
    PATH=$PATH:/opt/hadoop/bin:/opt/hadoop/sbin:/opt/spark/bin:/opt/spark/sbin \
    YARN_CONF_DIR=/opt/hadoop/etc/hadoop

RUN apt-get update && apt-get install -y --no-install-recommends \
    ssh \
    rsync \
    net-tools \
    curl \
    procps \
    vim \
    openssh-server \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

RUN useradd -ms /bin/bash -u 1000 hadoop && \
    echo "hadoop:hadoop" | chpasswd && \
    mkdir -p /home/hadoop/.ssh && \
    chmod 700 /home/hadoop/.ssh && \
    mkdir -p /opt/hadoop/logs && \
    mkdir -p /opt/spark/logs

RUN curl -L https://dlcdn.apache.org/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz | tar -xz -C /opt && \
    mv /opt/hadoop-$HADOOP_VERSION $HADOOP_HOME

RUN curl -L https://dlcdn.apache.org/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop3.tgz | tar -xz -C /opt && \
    mv /opt/spark-$SPARK_VERSION-bin-hadoop3 $SPARK_HOME

COPY ssh-keys/id_rsa.pub /home/hadoop/.ssh/authorized_keys
RUN chmod 600 /home/hadoop/.ssh/authorized_keys && \
    echo "Host *" >> /home/hadoop/.ssh/config && \
    echo "  StrictHostKeyChecking no" >> /home/hadoop/.ssh/config && \
    chmod 600 /home/hadoop/.ssh/config

RUN mkdir -p /var/run/sshd

COPY config/* $HADOOP_CONF_DIR/

RUN cp $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.conf && \
    cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh && \
    echo "export HADOOP_CONF_DIR=$HADOOP_CONF_DIR" >> $SPARK_HOME/conf/spark-env.sh && \
    echo "export YARN_CONF_DIR=$HADOOP_CONF_DIR" >> $SPARK_HOME/conf/spark-env.sh

RUN chown -R hadoop:hadoop /opt/hadoop /opt/spark /home/hadoop

WORKDIR /home/hadoop
USER hadoop

EXPOSE 22 8088 4040 9870 9864 9866 9868 10020 19888

CMD ["/usr/sbin/sshd", "-D"]
```

**Dockerfile.hadoop-lite (Hadoop Only)**

```python
FROM openjdk:8-jdk

ENV HADOOP_VERSION=3.3.6 \
    HADOOP_HOME=/opt/hadoop \
    HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop \
    JAVA_HOME=/usr/local/openjdk-8 \
    PATH=$PATH:/opt/hadoop/bin:/opt/hadoop/sbin

RUN apt-get update && apt-get install -y --no-install-recommends \
    ssh \
    rsync \
    net-tools \
    curl \
    procps \
    vim \
    openssh-server \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

RUN useradd -ms /bin/bash -u 1000 hadoop && \
    echo "hadoop:hadoop" | chpasswd && \
    mkdir -p /home/hadoop/.ssh && \
    chmod 700 /home/hadoop/.ssh && \
    mkdir -p /opt/hadoop/logs

RUN curl -L https://dlcdn.apache.org/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz | tar -xz -C /opt && \
    mv /opt/hadoop-$HADOOP_VERSION $HADOOP_HOME

COPY ssh-keys/id_rsa.pub /home/hadoop/.ssh/authorized_keys
RUN chmod 600 /home/hadoop/.ssh/authorized_keys && \
    echo "Host *" >> /home/hadoop/.ssh/config && \
    echo "  StrictHostKeyChecking no" >> /home/hadoop/.ssh/config && \
    chmod 600 /home/hadoop/.ssh/config

RUN mkdir -p /var/run/sshd

COPY config/* $HADOOP_CONF_DIR/

RUN chown -R hadoop:hadoop /opt/hadoop /home/hadoop

WORKDIR /home/hadoop
USER hadoop

EXPOSE 22 9000 9870 9866 9864 9868

CMD ["/usr/sbin/sshd", "-D"]
```

# **6. Docker 이미지 빌드**

기준 서버 (~/hadoop-cluster)에서 실행.

```python
docker build -t hadoop-spark-base:3.3.6-3.5.5 -f Dockerfile.hadoop-base .
docker build -t hadoop-lite:3.3.6 -f Dockerfile.hadoop-lite .
```

**7. Docker Compose 파일 작성 (docker-compose.yml)**

> 아래 extra_hosts 섹션의 <..._IP> 플레이스홀더를 **사용자 환경의 실제 서버 IP 주소**로 변경
> 

1. **사용자 정의 네트워크 생성 (모든 호스트에서 실행):**

```python
docker network create hadoop-net
```

1. **NameNode/RM 서버용 docker-compose.yml**

```python
version: '3.8'

services:
  namenode:
    image: hadoop-lite:3.3.6
    container_name: namenode
    hostname: namenode
    volumes:
      - ./data/namenode:/opt/hadoop/hdfs/namenode
      - ./ssh-keys/id_rsa:/home/hadoop/.ssh/id_rsa:ro
      - ./config/workers:/opt/hadoop/etc/hadoop/workers:ro
    ports:
      - "9870:9870"
      - "9000:9000"
      - "2222:22"
    networks:
      - hadoop-net
    extra_hosts:
      - "resourcemanager:<NameNode_RM_Server_IP>" # 자신의 IP
      - "worker1:<Worker1_Server_IP>"
      - "worker2:<Worker2_Server_IP>"
      - "secondarynamenode:<SNN_API_Server_IP>"

  resourcemanager:
    image: hadoop-spark-base:3.3.6-3.5.5
    container_name: resourcemanager
    hostname: resourcemanager
    depends_on:
      - namenode
    ports:
      - "8088:8088"
      - "8030:8030"
      - "8031:8031"
      - "8032:8032"
      - "10020:10020"
      - "19888:19888"
      - "4040:4040"
      - "2223:22"
    networks:
      - hadoop-net
    extra_hosts:
      - "namenode:<NameNode_RM_Server_IP>"
      - "worker1:<Worker1_Server_IP>"
      - "worker2:<Worker2_Server_IP>"
      - "secondarynamenode:<SNN_API_Server_IP>"

networks:
  hadoop-net:
    external: true
```

3. **Worker1 서버용 docker-compose.yml**

```python
version: '3.8'

services:
  worker1:
    image: hadoop-spark-base:3.3.6-3.5.5
    container_name: worker1
    hostname: worker1
    volumes:
      - ./data/datanode-w1:/opt/hadoop/hdfs/datanode
      - ./data/nm-local-w1:/opt/hadoop/tmp/nmlocaldir
      - ./data/nm-logs-w1:/opt/hadoop/logs/userlogs
    ports:
      - "9864:9864"
      - "9866:9866"
      - "8042:8042"
      - "2224:22"
    networks:
      - hadoop-net
    extra_hosts:
      - "namenode:<NameNode_RM_Server_IP>"
      - "resourcemanager:<NameNode_RM_Server_IP>"
      - "worker2:<Worker2_Server_IP>"
      - "secondarynamenode:<SNN_API_Server_IP>"

networks:
  hadoop-net:
    external: true

```

**Worker2 서버용 docker-compose.yml**

```python
version: '3.8'

services:
  secondarynamenode:
    image: hadoop-lite:3.3.6
    container_name: secondarynamenode
    hostname: secondarynamenode
    ports:
      - "9868:9868"
      - "2226:22"
    networks:
      - hadoop-net
    extra_hosts:
      - "namenode:<NameNode_RM_Server_IP>"
      - "resourcemanager:<NameNode_RM_Server_IP>"
      - "worker1:<Worker1_Server_IP>"
      - "worker2:<Worker2_Server_IP>"

  # api-server:
  #   image: your-api-image:latest
  #   container_name: api-server
  #   ports:
  #     - "8080:8080"
  #   networks:
  #     - hadoop-net
  #   extra_hosts:
  #     - "namenode:<NameNode_RM_Server_IP>"
  #     - "resourcemanager:<NameNode_RM_Server_IP>"
  #     - "worker1:<Worker1_Server_IP>"
  #     - "worker2:<Worker2_Server_IP>"
  #     - "secondarynamenode:<SNN_API_Server_IP>"

networks:
  hadoop-net:
    external: true
```

# 8. 클러스터 배포 및 실행

1. **파일 배포:**
    - ~/hadoop-cluster 내용 (config/, ssh-keys/, Dockerfile.*, 각 서버용 docker-compose.yml)을 해당 서버 ~/hadoop-cluster로 복사 (scp 사용).
    - ssh-keys/id_rsa는 NameNode/RM 서버에만 필요.
    - 빌드된 Docker 이미지를 각 서버에 로드.
2. **컨테이너 실행 (각 서버):** 각 서버의 ~/hadoop-cluster에서 실행. **IP 플레이스홀더를 실제 값으로 변경했는지 확인.**

```python
docker compose up -d
```

1. **NameNode 포맷 (최초 1회, NameNode/RM 서버):**

```python
docker exec -it -u hadoop namenode bash
hdfs namenode -format
exit
```

1. **Hadoop/YARN 서비스 시작 (NameNode/RM 서버)**

```python
docker exec -it -u hadoop namenode bash
start-dfs.sh
start-yarn.sh
mapred --daemon start historyserver
exit
```

# **9. 클러스터 상태 확인 및 검증**

1. **Web UI 확인:** <..._IP>를 실제 서버 IP로 변경하여 접속.
    - NN: http://<NameNode_RM_Server_IP>:9870
    - RM: http://<NameNode_RM_Server_IP>:8088
    - JobHistory: http://<NameNode_RM_Server_IP>:19888
    - Worker1 DN: http://<Worker1_Server_IP>:9864
    - Worker1 NM: http://<Worker1_Server_IP>:8042
    - Worker2 DN: http://<Worker2_Server_IP>:9864
    - Worker2 NM: http://<Worker2_Server_IP>:8042
    - SNN: http://<SNN_API_Server_IP>:9868

1. **컨테이너 프로세스 확인 (jps):**

```python
docker exec -u hadoop namenode jps
docker exec -u hadoop resourcemanager jps
docker exec -u hadoop worker1 jps
docker exec -u hadoop worker2 jps
docker exec -u hadoop secondarynamenode jps
```

1. **HDFS 명령어 테스트 (예: NameNode 컨테이너):**

```python
docker exec -it -u hadoop namenode bash
hdfs dfs -ls /
hdfs dfs -mkdir /test && echo "Hello" > test.txt && hdfs dfs -put test.txt /test/
hdfs dfs -cat /test/test.txt
hdfs dfs -rm -r /test && rm test.txt
exit
```

1. **Spark 예제 실행 (예: ResourceManager 컨테이너):**

```python
docker exec -it -u hadoop resourcemanager bash
spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 2 \
  --executor-memory 1G \
  --executor-cores 1 \
  $SPARK_HOME/examples/jars/spark-examples*.jar \
  10
exit
```

---

# BE

## Spring Boot 설정

```python
spring:
  application:
    name:
      BE
  jwt:
    secret: "OnetwothreeDroptheBeat134vasndjnaiosuervoiuasoeruvnasopieurvanpsoierunaspoieruvnaposieurn"
  data:
    redis:
      port: 6379
      host: localhost
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://localhost:3306/hello?serverTimezone=UTC # 이거 로컬 DB임
    username: root
    password: root
    hikari:
      pool-name: hikari
      maximum-pool-size: 1000
  jpa:
    show-sql: true
    hibernate:
      ddl-auto: validate
      naming:
        physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
    open-in-view: true #자잘한 경고 뜨는 거 때문에 설정 명시
    properties:
      hibernate:
        format_sql: true
        show_sql: true
        default_batch_fetch_size: 1000
logging.level:
  org.hibernate.SQL: debug

springdoc:
  default-consumes-media-type: application/json
  default-produces-media-type: application/json
  api-docs:
    groups:
      enabled: true
  swagger-ui:
    operations-sorter: alpha # alpha(알파벳 오름차순), method(HTTP메소드순)
    tags-sorter: alpha # 태그 정렬 기준
    path: /v3/api-docs/swagger-ui # html 문서 접속 경로
    #    disable-swagger-default-url: true
    #    display-query-params-without-oauth2: true
    doc-expansion: none # tag, operation 펼치는 방식
  paths-to-match:
    /**
```

---

# FE

## 1. git clone

<aside>

git clone https://lab.ssafy.com/s12-bigdata-dist-sub1/S12P21B210.git

</aside>

## 2. 어플리케이션 빌드 / FE 폴더

<aside>

1. yarn install
2. yarn androidS
</aside>